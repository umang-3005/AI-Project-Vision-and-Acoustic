{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e162fc",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition (SER) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb4b29",
   "metadata": {},
   "source": [
    "## Dataset downloading from kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c8560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAVDESS: /home/d8a0/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1\n",
      "CREMA-D: /home/d8a0/.cache/kagglehub/datasets/ejlok1/cremad/versions/1\n",
      "MELD: /home/d8a0/.cache/kagglehub/datasets/brij041/meld-with-audio-files/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download (cached automatically by kagglehub)\n",
    "ravdess_path = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\n",
    "crema_path   = kagglehub.dataset_download(\"ejlok1/cremad\")\n",
    "meld_path    = kagglehub.dataset_download(\"brij041/meld-with-audio-files\")\n",
    "\n",
    "print(\"RAVDESS:\", ravdess_path)\n",
    "print(\"CREMA-D:\", crema_path)\n",
    "print(\"MELD:\", meld_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c9cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "ROOT: /home/d8a0/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1\n",
      "Audio files: 2880\n",
      "CSV files: 0\n",
      "\n",
      "Sample audio paths:\n",
      " - Actor_23/03-01-03-01-02-01-23.wav\n",
      " - audio_speech_actors_01-24/Actor_22/03-01-07-01-01-02-22.wav\n",
      " - Actor_22/03-01-03-01-02-01-22.wav\n",
      " - Actor_08/03-01-03-02-02-02-08.wav\n",
      " - Actor_20/03-01-04-02-02-01-20.wav\n",
      " - audio_speech_actors_01-24/Actor_15/03-01-02-02-01-02-15.wav\n",
      " - Actor_19/03-01-07-02-01-02-19.wav\n",
      " - audio_speech_actors_01-24/Actor_02/03-01-02-01-01-01-02.wav\n",
      "\n",
      "==========================================================================================\n",
      "ROOT: /home/d8a0/.cache/kagglehub/datasets/ejlok1/cremad/versions/1\n",
      "Audio files: 7442\n",
      "CSV files: 0\n",
      "\n",
      "Sample audio paths:\n",
      " - AudioWAV/1024_MTI_NEU_XX.wav\n",
      " - AudioWAV/1036_IWL_DIS_XX.wav\n",
      " - AudioWAV/1017_TIE_DIS_XX.wav\n",
      " - AudioWAV/1078_MTI_FEA_XX.wav\n",
      " - AudioWAV/1081_ITH_FEA_XX.wav\n",
      " - AudioWAV/1060_IWL_ANG_XX.wav\n",
      " - AudioWAV/1054_ITH_HAP_XX.wav\n",
      " - AudioWAV/1075_IEO_FEA_MD.wav\n",
      "\n",
      "==========================================================================================\n",
      "ROOT: /home/d8a0/.cache/kagglehub/datasets/brij041/meld-with-audio-files/versions/1\n",
      "Audio files: 15908\n",
      "CSV files: 3\n",
      "\n",
      "Sample audio paths:\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/audio/train/dia67_utt2.wav\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/audio/test/dia39_utt5.wav\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/audio/train/dia109_utt6.wav\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/audio/train/dia40_utt10.wav\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/audio/train/dia789_utt1.wav\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/audio/test/final_videos_testdia51_utt0.wav\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/audio/train/dia666_utt2.wav\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/audio/test/dia173_utt10.wav\n",
      "\n",
      "Sample CSV paths:\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/dev_sent_emo.csv\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/test_sent_emo.csv\n",
      " - meld-dataset/MELD-RAW/MELD.Raw/train/train_sent_emo.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "AUDIO_EXTS = (\".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\")\n",
    "\n",
    "def inspect_dataset(root):\n",
    "    root = Path(root)\n",
    "    audio_files = [p for p in root.rglob(\"*\") if p.suffix.lower() in AUDIO_EXTS]\n",
    "    csv_files = list(root.rglob(\"*.csv\"))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"ROOT:\", root)\n",
    "    print(\"Audio files:\", len(audio_files))\n",
    "    print(\"CSV files:\", len(csv_files))\n",
    "\n",
    "    if audio_files:\n",
    "        sample = random.sample(audio_files, k=min(8, len(audio_files)))\n",
    "        print(\"\\nSample audio paths:\")\n",
    "        for p in sample:\n",
    "            print(\" -\", p.relative_to(root))\n",
    "\n",
    "    if csv_files:\n",
    "        print(\"\\nSample CSV paths:\")\n",
    "        for p in csv_files[:8]:\n",
    "            print(\" -\", p.relative_to(root))\n",
    "\n",
    "    return audio_files, csv_files\n",
    "\n",
    "rav_audio, _ = inspect_dataset(ravdess_path)\n",
    "cre_audio, _ = inspect_dataset(crema_path)\n",
    "mel_audio, mel_csv = inspect_dataset(meld_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed8735",
   "metadata": {},
   "source": [
    "## Define labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global label set used everywhere (training + mic adaptation + inference)\n",
    "LABELS = [\"angry\", \"fear\", \"happy\", \"neutral\", \"sad\", \"surprise\"]\n",
    "label2id = {l:i for i,l in enumerate(LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d16f7",
   "metadata": {},
   "source": [
    "## Deduplicate by filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c930a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAVDESS: total 2880 | unique filenames 1440 | duplicate rows 2880\n",
      "Example duplicate filename: 03-01-07-02-02-02-03.wav\n",
      "  - /home/d8a0/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1/Actor_03/03-01-07-02-02-02-03.wav\n",
      "  - /home/d8a0/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1/audio_speech_actors_01-24/Actor_03/03-01-07-02-02-02-03.wav\n",
      "\n",
      "CREMA-D: total 7442 | unique filenames 7442 | duplicate rows 0\n",
      "\n",
      "After dedup -> RAVDESS: 1440 files\n",
      "After dedup -> CREMA-D: 7442 files\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def duplicate_report_by_filename(files):\n",
    "    fn2paths = defaultdict(list)\n",
    "    for p in files:\n",
    "        fn2paths[p.name].append(p)\n",
    "\n",
    "    total = len(files)\n",
    "    unique = len(fn2paths)\n",
    "    dup_rows = sum(len(v) for v in fn2paths.values() if len(v) > 1)\n",
    "\n",
    "    example = None\n",
    "    for fn, paths in fn2paths.items():\n",
    "        if len(paths) > 1:\n",
    "            example = (fn, [str(p) for p in paths[:5]])\n",
    "            break\n",
    "\n",
    "    return total, unique, dup_rows, example\n",
    "\n",
    "def dedup_by_filename(files, prefer_substrings=None):\n",
    "    prefer_substrings = prefer_substrings or []\n",
    "    # choose preferred paths first, then keep first occurrence per filename\n",
    "    def pref_score(p):\n",
    "        s = str(p)\n",
    "        for i, sub in enumerate(prefer_substrings):\n",
    "            if sub in s:\n",
    "                return i\n",
    "        return len(prefer_substrings) + 1\n",
    "\n",
    "    files_sorted = sorted(files, key=pref_score)\n",
    "    seen = set()\n",
    "    kept = []\n",
    "    for p in files_sorted:\n",
    "        if p.name in seen:\n",
    "            continue\n",
    "        seen.add(p.name)\n",
    "        kept.append(p)\n",
    "    return kept\n",
    "\n",
    "rav_total, rav_unique, rav_dup, rav_ex = duplicate_report_by_filename(rav_audio)\n",
    "cre_total, cre_unique, cre_dup, cre_ex = duplicate_report_by_filename(cre_audio)\n",
    "\n",
    "print(\"RAVDESS: total\", rav_total, \"| unique filenames\", rav_unique, \"| duplicate rows\", rav_dup)\n",
    "if rav_ex:\n",
    "    print(\"Example duplicate filename:\", rav_ex[0])\n",
    "    for s in rav_ex[1]:\n",
    "        print(\"  -\", s)\n",
    "\n",
    "print(\"\\nCREMA-D: total\", cre_total, \"| unique filenames\", cre_unique, \"| duplicate rows\", cre_dup)\n",
    "if cre_ex:\n",
    "    print(\"Example duplicate filename:\", cre_ex[0])\n",
    "    for s in cre_ex[1]:\n",
    "        print(\"  -\", s)\n",
    "\n",
    "# Deduplicate (keep one copy per filename)\n",
    "rav_audio_unique = dedup_by_filename(rav_audio, prefer_substrings=[\"audio_speech_actors_01-24\"])\n",
    "cre_audio_unique = dedup_by_filename(cre_audio, prefer_substrings=[\"AudioWAV\"])\n",
    "\n",
    "print(\"\\nAfter dedup -> RAVDESS:\", len(rav_audio_unique), \"files\")\n",
    "print(\"After dedup -> CREMA-D:\", len(cre_audio_unique), \"files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c527e",
   "metadata": {},
   "source": [
    "## Build manifests (RAVDESS + CREMA-D) from filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53454ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAVDESS kept: 1248\n",
      "label\n",
      "neutral     288\n",
      "fear        192\n",
      "surprise    192\n",
      "angry       192\n",
      "sad         192\n",
      "happy       192\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CREMA-D kept: 6171\n",
      "label\n",
      "happy      1271\n",
      "fear       1271\n",
      "angry      1271\n",
      "sad        1271\n",
      "neutral    1087\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TARGET = {\"happy\", \"sad\", \"angry\", \"neutral\", \"surprise\", \"fear\"}\n",
    "\n",
    "# RAVDESS filename: MM-VC-EM-INT-STAT-REP-ACTOR.wav\n",
    "# EM: 01 neutral, 02 calm, 03 happy, 04 sad, 05 angry, 06 fearful, 07 disgust, 08 surprised\n",
    "RAV_EMO = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"neutral\",   # calm -> neutral\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fear\",\n",
    "    \"07\": None,        # disgust -> drop\n",
    "    \"08\": \"surprise\",\n",
    "}\n",
    "\n",
    "# CREMA-D filename: ActorID_SentenceID_Emotion_Level.wav (e.g., 1053_IEO_ANG_HI.wav)\n",
    "CREMA_EMO = {\n",
    "    \"ANG\": \"angry\",\n",
    "    \"HAP\": \"happy\",\n",
    "    \"SAD\": \"sad\",\n",
    "    \"FEA\": \"fear\",\n",
    "    \"NEU\": \"neutral\",\n",
    "    \"DIS\": None,       # drop\n",
    "}\n",
    "\n",
    "def build_ravdess_manifest_from_files(files):\n",
    "    rows = []\n",
    "    for wav in files:\n",
    "        parts = wav.stem.split(\"-\")\n",
    "        if len(parts) != 7:\n",
    "            continue\n",
    "        emo_code = parts[2]\n",
    "        actor_id = parts[6]\n",
    "        label = RAV_EMO.get(emo_code, None)\n",
    "        if label is None or label not in TARGET:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"path\": str(wav),\n",
    "            \"dataset\": \"ravdess\",\n",
    "            \"speaker_id\": f\"rav_actor_{actor_id}\",\n",
    "            \"raw_label\": emo_code,\n",
    "            \"label\": label\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_crema_manifest_from_files(files):\n",
    "    rows = []\n",
    "    for wav in files:\n",
    "        parts = wav.stem.split(\"_\")\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "        actor_id = parts[0]\n",
    "        emo_code = parts[2]\n",
    "        label = CREMA_EMO.get(emo_code, None)\n",
    "        if label is None or label not in TARGET:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"path\": str(wav),\n",
    "            \"dataset\": \"crema_d\",\n",
    "            \"speaker_id\": f\"cre_actor_{actor_id}\",\n",
    "            \"raw_label\": emo_code,\n",
    "            \"label\": label\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "rav_df = build_ravdess_manifest_from_files(rav_audio_unique)\n",
    "cre_df = build_crema_manifest_from_files(cre_audio_unique)\n",
    "\n",
    "print(\"RAVDESS kept:\", len(rav_df))\n",
    "print(rav_df[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nCREMA-D kept:\", len(cre_df))\n",
    "print(cre_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638494dc",
   "metadata": {},
   "source": [
    "## Build MELD manifest (labels from CSV + audio from `audio/{train,dev,test}`)\n",
    "\n",
    "### MELD audio files are named like: `dia{Dialogue_ID}_utt{Utterance_ID}.wav`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb14272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[train] total CSV rows: 9989\n",
      "[train] kept: 9718 | missing_audio: 0 | dropped(not in 6): 271\n",
      "[train] label counts:\n",
      " label\n",
      "neutral     4710\n",
      "happy       1743\n",
      "surprise    1205\n",
      "angry       1109\n",
      "sad          683\n",
      "fear         268\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[dev] total CSV rows: 1109\n",
      "[dev] kept: 1086 | missing_audio: 1 | dropped(not in 6): 22\n",
      "[dev] label counts:\n",
      " label\n",
      "neutral     469\n",
      "happy       163\n",
      "angry       153\n",
      "surprise    150\n",
      "sad         111\n",
      "fear         40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[test] total CSV rows: 2610\n",
      "[test] kept: 2542 | missing_audio: 0 | dropped(not in 6): 68\n",
      "[test] label counts:\n",
      " label\n",
      "neutral     1256\n",
      "happy        402\n",
      "angry        345\n",
      "surprise     281\n",
      "sad          208\n",
      "fear          50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "TOTAL MELD kept: 13346\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "meld_root = Path(meld_path) / \"meld-dataset\" / \"MELD-RAW\" / \"MELD.Raw\"\n",
    "\n",
    "train_csv = meld_root / \"train\" / \"train_sent_emo.csv\"\n",
    "dev_csv   = meld_root / \"dev_sent_emo.csv\"\n",
    "test_csv  = meld_root / \"test_sent_emo.csv\"\n",
    "\n",
    "audio_train = meld_root / \"audio\" / \"train\"\n",
    "audio_dev   = meld_root / \"audio\" / \"dev\"\n",
    "audio_test  = meld_root / \"audio\" / \"test\"\n",
    "\n",
    "MELD_MAP = {\n",
    "    \"anger\": \"angry\",\n",
    "    \"sadness\": \"sad\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"surprise\": \"surprise\",\n",
    "    \"fear\": \"fear\",\n",
    "    \"joy\": \"happy\",\n",
    "    \"disgust\": None,   # drop\n",
    "}\n",
    "\n",
    "def build_meld_manifest(csv_path: Path, audio_dir: Path, split_name: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    rows = []\n",
    "    missing_audio = 0\n",
    "    dropped_label = 0\n",
    "\n",
    "    for _, r in df.iterrows():\n",
    "        raw = str(r[\"Emotion\"]).strip().lower()\n",
    "        mapped = MELD_MAP.get(raw, None)\n",
    "        if mapped is None:\n",
    "            dropped_label += 1\n",
    "            continue\n",
    "\n",
    "        did = int(r[\"Dialogue_ID\"])\n",
    "        uid = int(r[\"Utterance_ID\"])\n",
    "        fname = f\"dia{did}_utt{uid}.wav\"\n",
    "        fpath = audio_dir / fname\n",
    "\n",
    "        if not fpath.exists():\n",
    "            missing_audio += 1\n",
    "            continue\n",
    "\n",
    "        speaker = str(r[\"Speaker\"]).strip()\n",
    "\n",
    "        rows.append({\n",
    "            \"path\": str(fpath),\n",
    "            \"dataset\": \"meld\",\n",
    "            \"speaker_id\": f\"meld_{speaker}\",\n",
    "            \"raw_label\": raw,\n",
    "            \"label\": mapped,\n",
    "            \"split_original\": split_name,\n",
    "            \"dialogue_id\": did,\n",
    "            \"utterance_id\": uid\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    print(f\"\\n[{split_name}] total CSV rows: {len(df)}\")\n",
    "    print(f\"[{split_name}] kept: {len(out)} | missing_audio: {missing_audio} | dropped(not in 6): {dropped_label}\")\n",
    "    if len(out) > 0:\n",
    "        print(f\"[{split_name}] label counts:\\n\", out[\"label\"].value_counts())\n",
    "    else:\n",
    "        print(f\"[{split_name}] WARNING: kept 0 rows. Check paths.\")\n",
    "\n",
    "    return out\n",
    "\n",
    "meld_train_df = build_meld_manifest(train_csv, audio_train, \"train\")\n",
    "meld_dev_df   = build_meld_manifest(dev_csv, audio_dev, \"dev\")\n",
    "meld_test_df  = build_meld_manifest(test_csv, audio_test, \"test\")\n",
    "\n",
    "meld_df = pd.concat([meld_train_df, meld_dev_df, meld_test_df], ignore_index=True)\n",
    "print(\"\\nTOTAL MELD kept:\", len(meld_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2132a4",
   "metadata": {},
   "source": [
    "## Save individual manifests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2138be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - manifests/ravdess_manifest.csv\n",
      " - manifests/crema_manifest.csv\n",
      " - manifests/meld_manifest.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"manifests\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "rav_df.to_csv(out_dir / \"ravdess_manifest.csv\", index=False)\n",
    "cre_df.to_csv(out_dir / \"crema_manifest.csv\", index=False)\n",
    "meld_df.to_csv(out_dir / \"meld_manifest.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", out_dir / \"ravdess_manifest.csv\")\n",
    "print(\" -\", out_dir / \"crema_manifest.csv\")\n",
    "print(\" -\", out_dir / \"meld_manifest.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245b95d",
   "metadata": {},
   "source": [
    "## Combine all three, then split train/val/test by **speaker groups** (random ratios)\n",
    "\n",
    "### This guarantees **no speaker overlap** across splits *after combining*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e779a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split counts:\n",
      " split\n",
      "train    16529\n",
      "val       3077\n",
      "test      1159\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label counts:\n",
      " label\n",
      "neutral     7810\n",
      "happy       3771\n",
      "angry       3070\n",
      "sad         2465\n",
      "surprise    1828\n",
      "fear        1821\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Speaker overlap checks (must be 0):\n",
      "train∩val : 0\n",
      "train∩test: 0\n",
      "val∩test  : 0\n",
      "\n",
      " Saved: manifests/final_manifest_speaker_disjoint_ratio.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "# Combine (keep only required columns)\n",
    "keep_cols = [\"path\", \"dataset\", \"speaker_id\", \"raw_label\", \"label\"]\n",
    "all_df = pd.concat([\n",
    "    rav_df[keep_cols],\n",
    "    cre_df[keep_cols],\n",
    "    meld_df[keep_cols]\n",
    "], ignore_index=True).drop_duplicates(\"path\").reset_index(drop=True)\n",
    "\n",
    "# Make speaker IDs globally unique across datasets\n",
    "all_df[\"group_speaker\"] = all_df[\"dataset\"].astype(str) + \"__\" + all_df[\"speaker_id\"].astype(str)\n",
    "\n",
    "# Random speaker-level split by ratios\n",
    "train_ratio, val_ratio, test_ratio = 0.80, 0.10, 0.10\n",
    "speakers = all_df[\"group_speaker\"].unique().tolist()\n",
    "rng.shuffle(speakers)\n",
    "\n",
    "n = len(speakers)\n",
    "n_train = int(round(train_ratio * n))\n",
    "n_val   = int(round(val_ratio * n))\n",
    "\n",
    "train_sp = set(speakers[:n_train])\n",
    "val_sp   = set(speakers[n_train:n_train+n_val])\n",
    "test_sp  = set(speakers[n_train+n_val:])\n",
    "\n",
    "def assign_split(gs):\n",
    "    if gs in train_sp: return \"train\"\n",
    "    if gs in val_sp:   return \"val\"\n",
    "    return \"test\"\n",
    "\n",
    "all_df[\"split\"] = all_df[\"group_speaker\"].map(assign_split)\n",
    "\n",
    "print(\"Split counts:\\n\", all_df[\"split\"].value_counts())\n",
    "print(\"\\nLabel counts:\\n\", all_df[\"label\"].value_counts())\n",
    "\n",
    "# Leak checks \n",
    "print(\"\\nSpeaker overlap checks (must be 0):\")\n",
    "print(\"train∩val :\", len(train_sp & val_sp))\n",
    "print(\"train∩test:\", len(train_sp & test_sp))\n",
    "print(\"val∩test  :\", len(val_sp & test_sp))\n",
    "\n",
    "# Save final\n",
    "out_dir = Path(\"manifests\"); out_dir.mkdir(exist_ok=True)\n",
    "final_path = out_dir / \"final_manifest_speaker_disjoint_ratio.csv\"\n",
    "all_df.to_csv(final_path, index=False)\n",
    "print(\"\\n Saved:\", final_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe0160",
   "metadata": {},
   "source": [
    "## Check for imbalance per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label  angry  fear  happy  neutral   sad  surprise\n",
      "split                                             \n",
      "test     205   188    208      291   199        68\n",
      "train   2438  1401   2991     6282  1954      1463\n",
      "val      427   232    572     1237   312       297\n",
      "\n",
      "Percent per split:\n",
      "label  angry   fear  happy  neutral    sad  surprise\n",
      "split                                               \n",
      "test   17.69  16.22  17.95    25.11  17.17      5.87\n",
      "train  14.75   8.48  18.10    38.01  11.82      8.85\n",
      "val    13.88   7.54  18.59    40.20  10.14      9.65\n",
      "train: neutral-rate baseline accuracy = 0.380\n",
      "val: neutral-rate baseline accuracy = 0.402\n",
      "test: neutral-rate baseline accuracy = 0.251\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_df = pd.read_csv(\"manifests/final_manifest_speaker_disjoint_ratio.csv\")\n",
    "\n",
    "ct = pd.crosstab(final_df[\"split\"], final_df[\"label\"])\n",
    "print(ct)\n",
    "\n",
    "print(\"\\nPercent per split:\")\n",
    "print((ct.div(ct.sum(axis=1), axis=0) * 100).round(2))\n",
    "\n",
    "# Baseline \"always neutral\" accuracy per split \n",
    "for sp in [\"train\",\"val\",\"test\"]:\n",
    "    sub = final_df[final_df[\"split\"]==sp]\n",
    "    neutral_rate = (sub[\"label\"]==\"neutral\").mean()\n",
    "    print(f\"{sp}: neutral-rate baseline accuracy = {neutral_rate:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e8865",
   "metadata": {},
   "source": [
    "## Weighted imbalance classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb2760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: ['angry', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "counts: {'angry': 2438, 'fear': 1401, 'happy': 2991, 'neutral': 6282, 'sad': 1954, 'surprise': 1463}\n",
      "weights: {'angry': 1.13, 'fear': 1.966, 'happy': 0.921, 'neutral': 0.439, 'sad': 1.41, 'surprise': 1.883}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_df = final_df[final_df[\"split\"]==\"train\"]\n",
    "labels = sorted(train_df[\"label\"].unique())\n",
    "counts = train_df[\"label\"].value_counts().reindex(labels).values\n",
    "\n",
    "# Inverse-frequency weights \n",
    "weights = counts.sum() / (len(labels) * counts)\n",
    "\n",
    "label2id_tmp = {lab:i for i, lab in enumerate(labels)}\n",
    "print(\"labels:\", labels)\n",
    "print(\"counts:\", dict(zip(labels, counts)))\n",
    "print(\"weights:\", dict(zip(labels, weights.round(3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f69a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, y_true = eval_pred\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2192654",
   "metadata": {},
   "source": [
    "## WavLM training setup (weighted loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "301d0363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets module: <module 'datasets' from '/home/d8a0/AI_Project/env/lib/python3.12/site-packages/datasets/__init__.py'>\n",
      "datasets __file__: /home/d8a0/AI_Project/env/lib/python3.12/site-packages/datasets/__init__.py\n",
      "sys.path[0]: \n"
     ]
    }
   ],
   "source": [
    "import datasets, sys\n",
    "print(\"datasets module:\", datasets)\n",
    "print(\"datasets __file__:\", getattr(datasets, \"__file__\", None))\n",
    "print(\"sys.path[0]:\", sys.path[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cbfa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train counts: {'angry': 2438, 'fear': 1401, 'happy': 2991, 'neutral': 6282, 'sad': 1954, 'surprise': 1463}\n",
      "Class weights: {'angry': 1.13, 'fear': 1.966, 'happy': 0.921, 'neutral': 0.439, 'sad': 1.41, 'surprise': 1.883}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForSequenceClassification were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10335' max='10335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10335/10335 29:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.368300</td>\n",
       "      <td>1.370193</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.485319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.190100</td>\n",
       "      <td>1.289811</td>\n",
       "      <td>0.501137</td>\n",
       "      <td>0.487298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.097300</td>\n",
       "      <td>1.209045</td>\n",
       "      <td>0.567436</td>\n",
       "      <td>0.555547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.042600</td>\n",
       "      <td>1.250443</td>\n",
       "      <td>0.546961</td>\n",
       "      <td>0.537144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.928500</td>\n",
       "      <td>1.277703</td>\n",
       "      <td>0.561911</td>\n",
       "      <td>0.545677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10335, training_loss=1.1697039851025792, metrics={'train_runtime': 1789.3381, 'train_samples_per_second': 46.187, 'train_steps_per_second': 5.776, 'total_flos': 3.8163157817943e+18, 'train_loss': 1.1697039851025792, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contextlib\n",
    "import numpy as np\n",
    "if not hasattr(np, \"_no_nep50_warning\"):\n",
    "    np._no_nep50_warning = contextlib.nullcontext\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from transformers import AutoFeatureExtractor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available. You said you're using GPU—fix that first.\"\n",
    "\n",
    "kw = inspect.signature(TrainingArguments.__init__).parameters\n",
    "eval_key = \"eval_strategy\" if \"eval_strategy\" in kw else \"evaluation_strategy\"\n",
    "\n",
    "# 1) Load manifest\n",
    "df = pd.read_csv(\"manifests/final_manifest_speaker_disjoint_ratio.csv\")\n",
    "\n",
    "labels = LABELS  # use global label order\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id).astype(int)\n",
    "\n",
    "train_df = df[df[\"split\"]==\"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"]==\"val\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"]==\"test\"].reset_index(drop=True)\n",
    "\n",
    "# 2) Class weights from TRAIN only\n",
    "counts = train_df[\"label\"].value_counts().reindex(labels, fill_value=0).values\n",
    "counts = np.maximum(counts, 1)  # avoid divide-by-zero\n",
    "class_weights = (counts.sum() / (len(labels) * counts)).astype(np.float32)\n",
    "class_weights_t = torch.tensor(class_weights)\n",
    "\n",
    "print(\"Train counts:\", dict(zip(labels, counts)))\n",
    "print(\"Class weights:\", dict(zip(labels, class_weights.round(3))))\n",
    "\n",
    "# 3) HF datasets (paths only)\n",
    "train_ds_base = Dataset.from_pandas(train_df[[\"path\",\"label_id\"]])\n",
    "val_ds_base   = Dataset.from_pandas(val_df[[\"path\",\"label_id\"]])\n",
    "test_ds_base  = Dataset.from_pandas(test_df[[\"path\",\"label_id\"]])\n",
    "\n",
    "MODEL_NAME = \"microsoft/wavlm-base\"\n",
    "feat = AutoFeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(labels), label2id=label2id, id2label=id2label\n",
    ")\n",
    "\n",
    "TARGET_SR = 16000\n",
    "MAX_SECONDS = 6\n",
    "MAX_LEN = TARGET_SR * MAX_SECONDS\n",
    "\n",
    "def load_and_resample(path: str):\n",
    "    wav, sr = torchaudio.load(path)  # [C,T]\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    wav = wav.squeeze(0)\n",
    "    if sr != TARGET_SR:\n",
    "        wav = torchaudio.functional.resample(wav, sr, TARGET_SR)\n",
    "    if wav.numel() > MAX_LEN:\n",
    "        wav = wav[:MAX_LEN]\n",
    "    return wav.numpy()\n",
    "\n",
    "# Collator loads audio per batch \n",
    "def collate_fn_base(features):\n",
    "    audios = [load_and_resample(f[\"path\"]) for f in features]\n",
    "    y = torch.tensor([f[\"label_id\"] for f in features], dtype=torch.long)\n",
    "    inputs = feat(\n",
    "        audios,\n",
    "        sampling_rate=TARGET_SR,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs[\"labels\"] = y\n",
    "    return inputs\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        y = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=class_weights_t.to(outputs.logits.device))\n",
    "        loss = loss_fn(outputs.logits, y)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"wavlm_ser_ckpt\",\n",
    "    **{eval_key: \"epoch\"},\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,                 \n",
    "    dataloader_num_workers=4,  # speed up CPU audio loading\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    remove_unused_columns=False,  # IMPORTANT: keep \"path\"/\"label_id\" for collator\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds_base,\n",
    "    eval_dataset=val_ds_base,\n",
    "    data_collator=collate_fn_base,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab8e0d",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58dfac92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved BASE model to: /home/d8a0/AI_Project/wavlm_ser_base\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "BASE_DIR = \"wavlm_ser_base\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "trainer.save_model(BASE_DIR)\n",
    "feat.save_pretrained(BASE_DIR)\n",
    "with open(os.path.join(BASE_DIR, \"labels.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved BASE model to:\", os.path.abspath(BASE_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d2210",
   "metadata": {},
   "source": [
    "## Point to 60 labeled own dataset files and build a small manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f2841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: /home/d8a0/AI_Project/my_audio_16k\n",
      "Found labeled wav files: 60\n",
      "\n",
      "Label counts:\n",
      " label\n",
      "fear        10\n",
      "surprise    10\n",
      "neutral     10\n",
      "happy       10\n",
      "angry       10\n",
      "sad         10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Speaker counts:\n",
      " speaker\n",
      "colleague    30\n",
      "me           30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "MY_DATA_DIR = Path(r\"./my_audio_16k\")  \n",
    "\n",
    "# Exact label set \n",
    "label_set = set(LABELS)\n",
    "\n",
    "def parse_label_and_speaker(p: Path):\n",
    "    stem = p.stem.lower()  # e.g., \"dog_angry_u\"\n",
    "    tokens = stem.split(\"_\")\n",
    "\n",
    "    # label is one of the tokens\n",
    "    lab = next((t for t in tokens if t in label_set), None)\n",
    "\n",
    "    # speaker: \"_u\" means colleague, else \"me\"\n",
    "    speaker = \"colleague\" if \"u\" in tokens else \"me\"\n",
    "\n",
    "    return lab, speaker\n",
    "\n",
    "rows = []\n",
    "for p in MY_DATA_DIR.glob(\"*.wav\"):\n",
    "    lab, speaker = parse_label_and_speaker(p)\n",
    "    if lab is None:\n",
    "        continue\n",
    "    rows.append({\"path\": str(p), \"label\": lab, \"speaker\": speaker})\n",
    "\n",
    "df_adapt = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Folder:\", MY_DATA_DIR.resolve())\n",
    "print(\"Found labeled wav files:\", len(df_adapt))\n",
    "print(\"\\nLabel counts:\\n\", df_adapt[\"label\"].value_counts() if len(df_adapt) else \"NONE\")\n",
    "print(\"\\nSpeaker counts:\\n\", df_adapt[\"speaker\"].value_counts() if len(df_adapt) else \"NONE\")\n",
    "\n",
    "# show unlabeled files if any\n",
    "all_wavs = list(MY_DATA_DIR.glob(\"*.wav\"))\n",
    "unlabeled = [p.name for p in all_wavs if parse_label_and_speaker(p)[0] is None]\n",
    "if unlabeled:\n",
    "    print(\"\\n These files had no detectable label token:\", unlabeled[:20], (\"...\" if len(unlabeled)>20 else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036c06a",
   "metadata": {},
   "source": [
    "## Split train/val by speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f78789cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 30 | Val size: 30\n",
      "\n",
      "Train label counts:\n",
      " label\n",
      "fear        5\n",
      "surprise    5\n",
      "happy       5\n",
      "neutral     5\n",
      "angry       5\n",
      "sad         5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label counts:\n",
      " label\n",
      "fear        5\n",
      "surprise    5\n",
      "neutral     5\n",
      "angry       5\n",
      "happy       5\n",
      "sad         5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df_mic = df_adapt[df_adapt[\"speaker\"]==\"me\"].reset_index(drop=True)\n",
    "val_df_mic   = df_adapt[df_adapt[\"speaker\"]==\"colleague\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", len(train_df_mic), \"| Val size:\", len(val_df_mic))\n",
    "print(\"\\nTrain label counts:\\n\", train_df_mic[\"label\"].value_counts())\n",
    "print(\"\\nVal label counts:\\n\", val_df_mic[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f81d69",
   "metadata": {},
   "source": [
    "## Create a tiny dataset loader with light augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c432de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "TARGET_SR = 16000\n",
    "MAX_SECONDS = 6.0\n",
    "MAX_LEN = int(TARGET_SR * MAX_SECONDS)\n",
    "\n",
    "\n",
    "def load_wav(path):\n",
    "    wav, sr = torchaudio.load(path)\n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    wav = wav.squeeze(0)\n",
    "    if sr != TARGET_SR:\n",
    "        wav = torchaudio.functional.resample(wav, sr, TARGET_SR)\n",
    "    return wav\n",
    "\n",
    "def random_crop(wav):\n",
    "    if wav.numel() <= MAX_LEN:\n",
    "        return wav\n",
    "    start = torch.randint(0, wav.numel() - MAX_LEN + 1, (1,)).item()\n",
    "    return wav[start:start+MAX_LEN]\n",
    "\n",
    "def augment(wav):\n",
    "    # random gain + tiny noise\n",
    "    gain = 10 ** (torch.empty(1).uniform_(-0.3, 0.3).item())\n",
    "    wav = wav * gain\n",
    "    noise_level = torch.empty(1).uniform_(0.0, 0.01).item()\n",
    "    wav = wav + noise_level * torch.randn_like(wav)\n",
    "    return torch.clamp(wav, -1.0, 1.0)\n",
    "\n",
    "class AdaptDS(Dataset):\n",
    "    def __init__(self, df, train=True):\n",
    "        self.df = df\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.df.loc[idx, \"path\"]\n",
    "        y = label2id[self.df.loc[idx, \"label\"]]\n",
    "\n",
    "        wav = load_wav(path)\n",
    "        if self.train:\n",
    "            wav = random_crop(wav)\n",
    "            wav = augment(wav)\n",
    "        else:\n",
    "            if wav.numel() > MAX_LEN:\n",
    "                wav = wav[-MAX_LEN:]\n",
    "\n",
    "        return {\"audio\": wav.numpy().astype(np.float32), \"label_id\": y}\n",
    "\n",
    "train_ds_mic = AdaptDS(train_df_mic, train=True)\n",
    "val_ds_mic = AdaptDS(val_df_mic, train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3884b",
   "metadata": {},
   "source": [
    "## Freeze everything except classifier head and fine-tune with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed0e95ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: ['classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 32/240 00:06 < 00:45, 4.55 it/s, Epoch 4/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.335392</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.577941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.330691</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.577941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.327073</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.577941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.324772</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.577941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val metrics: {'eval_loss': 1.3353922367095947, 'eval_accuracy': 0.6333333333333333, 'eval_macro_f1': 0.5779405779405778, 'eval_runtime': 0.3555, 'eval_samples_per_second': 84.392, 'eval_steps_per_second': 22.505, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "kw = inspect.signature(TrainingArguments.__init__).parameters\n",
    "eval_key = \"eval_strategy\" if \"eval_strategy\" in kw else \"evaluation_strategy\"\n",
    "\n",
    "# Freeze everything\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Unfreeze classifier only\n",
    "trainable = []\n",
    "for name, p in model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        p.requires_grad = True\n",
    "        trainable.append(name)\n",
    "\n",
    "print(\"Trainable params:\", trainable)\n",
    "\n",
    "def collate_fn_mic(batch):\n",
    "    audios = [b[\"audio\"] for b in batch]\n",
    "    y = torch.tensor([b[\"label_id\"] for b in batch], dtype=torch.long)\n",
    "    inputs = feat(audios, sampling_rate=TARGET_SR, padding=True, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = y\n",
    "    return inputs\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, y_true = eval_pred\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "class HeadOnlyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        y = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, y)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"wavlm_head_adapt_ckpt\",\n",
    "    **{eval_key: \"epoch\"},\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.05,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=30,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer_adapt = HeadOnlyTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds_mic,\n",
    "    eval_dataset=val_ds_mic,\n",
    "    data_collator=collate_fn_mic,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer_adapt.train()\n",
    "print(\"Val metrics:\", trainer_adapt.evaluate())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49df01a",
   "metadata": {},
   "source": [
    "## Save the adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5af4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FINAL adapted model to: /home/d8a0/AI_Project/wavlm_ser_model\n",
      "Saved .pth to: /home/d8a0/AI_Project/wavlm_ser_model.pth\n",
      "Best checkpoint: wavlm_head_adapt_ckpt/checkpoint-8\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import torch\n",
    "\n",
    "SAVE_DIR = \"wavlm_ser_model\"  # final model for live testing\n",
    "PTH_PATH = os.path.abspath(\"wavlm_ser_model.pth\")\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# After trainer_adapt.train(), trainer_adapt.model is the final (often best) model\n",
    "trainer_adapt.save_model(SAVE_DIR)     # saves config + weights\n",
    "feat.save_pretrained(SAVE_DIR)         \n",
    "\n",
    "# Save FINAL adapted model weights as .pth (state_dict)\n",
    "torch.save(trainer_adapt.model.state_dict(), PTH_PATH)\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"labels.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved FINAL adapted model to:\", os.path.abspath(SAVE_DIR))\n",
    "print(\"Saved .pth to:\", os.path.abspath(PTH_PATH))\n",
    "print(\"Best checkpoint:\", getattr(trainer_adapt.state, \"best_model_checkpoint\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c109d51",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0adcda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9535818696022034,\n",
       " 'eval_accuracy': 0.6764452113891286,\n",
       " 'eval_macro_f1': 0.6635750220038531,\n",
       " 'eval_runtime': 6.6772,\n",
       " 'eval_samples_per_second': 173.576,\n",
       " 'eval_steps_per_second': 21.716,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_ds_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61d53470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d8a0/AI_Project/env/lib/python3.12/site-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry      0.639     0.854     0.731       205\n",
      "        fear      0.708     0.734     0.721       188\n",
      "       happy      0.635     0.644     0.640       208\n",
      "     neutral      0.774     0.601     0.677       291\n",
      "         sad      0.674     0.613     0.642       199\n",
      "    surprise      0.556     0.588     0.571        68\n",
      "\n",
      "    accuracy                          0.676      1159\n",
      "   macro avg      0.664     0.672     0.664      1159\n",
      "weighted avg      0.684     0.676     0.675      1159\n",
      "\n",
      "[[175   4  16   5   3   2]\n",
      " [  4 138  15   4  26   1]\n",
      " [ 40  10 134  11   5   8]\n",
      " [ 46   5  21 175  24  20]\n",
      " [  4  34  11  27 122   1]\n",
      " [  5   4  14   4   1  40]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred = trainer.predict(test_ds_base)\n",
    "y_true = pred.label_ids\n",
    "y_pred = np.argmax(pred.predictions, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=labels, digits=3))\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
